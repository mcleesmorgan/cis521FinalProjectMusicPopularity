# -*- coding: utf-8 -*-
"""LyricsClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L1tsDuScQWxSmSXEu42Uh6XVine1s5zn
"""

import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import ElasticNet
from sklearn.decomposition import TruncatedSVD
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import MaxAbsScaler
from sklearn.metrics import accuracy_score  
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch
import nltk.tokenize
from nltk.probability import FreqDist
nltk.download("punkt")

#preprocess the lyrics and count the frequencies

corpus = []
y_data = []
y_lin_data = []
names = []
fileNames = ["SpotifyFeaturesLyricsEdited0.csv", "SpotifyFeaturesLyricsEdited1.csv", "SpotifyFeaturesLyricsEdited2.csv",\
             "SpotifyFeaturesLyricsEdited3.csv", "SpotifyFeaturesLyricsEdited4.csv", "SpotifyFeaturesLyricsEdited5.csv",\
             "SpotifyFeaturesLyricsEdited5.csv", "SpotifyFeaturesLyricsEdited6.csv", "SpotifyFeaturesLyricsEdited7.csv",\
             "SpotifyFeaturesLyricsEdited8.csv", "SpotifyFeaturesLyricsEdited9.csv", "SpotifyFeaturesLyricsEdited10.csv",\
             "SpotifyFeaturesLyricsEdited11.csv", "SpotifyFeaturesLyricsEdited12.csv", "SpotifyFeaturesLyricsEdited13.csv",\
             "SpotifyFeaturesLyricsEdited14.csv", "SpotifyFeaturesLyricsEdited15.csv"]

total = 0
for name in fileNames:
  df = pd.read_csv(name)
  for (index, row) in df.iterrows():
    lyr = nltk.word_tokenize(row['lyrics'])
    lyr = [l.lower() for l in lyr if l.isalpha()]
    y_data.append(row['popularity'])
    y_lin_data.append(row['popularity'])
    str1 = " "
    lyr = str1.join(lyr)
    corpus.append(lyr)
    names.append(row['track_name'])
  total = total + df['popularity'].mean()

threshold = 55
for i in range(len(y_data)):
  if y_data[i] >= threshold:
    y_data[i] = 1
  else:
    y_data[i] = 0

vectorizer = CountVectorizer(lowercase=True, strip_accents="ascii")
X_data = vectorizer.fit_transform(corpus)
names = {"track_name": names, "popularity": y_lin_data}
namesDf = pd.DataFrame(data=names)
# namesDf.to_csv("lyrics_track_names_popularity.csv")

X_lin, X_lin_test, y_lin, y_lin_test = train_test_split(X_data, y_lin_data, test_size = 0.2)

#logistic regression
lin_reg = ElasticNet(alpha=.5, l1_ratio = 0.4)
lin_reg = lin_reg.fit(X_lin, y_lin)
y_lin_pred = lin_reg.predict(X_lin_test)
print("linear regression score: ", lin_reg.score(X_lin_test, y_lin_test))
print("Linear regression mse: ", mean_squared_error(y_lin_test, y_lin_pred))

X_log, X_log_test, y_log, y_log_test = train_test_split(X_data, y_data, test_size = 0.2)

#normalize the data
scaler = MaxAbsScaler()
scaler.fit(X_log)
X_log = scaler.transform(X_log)
X_log_test = scaler.transform(X_log_test)

#perform PCA first
pca = TruncatedSVD(n_components=50, n_iter=10)
pca.fit(X_log)
X_log = pca.fit_transform(X_log)
X_log_test = pca.fit_transform(X_log_test)

#logistic regression
log_reg = LogisticRegression(penalty='l2', solver='liblinear')
log_reg = log_reg.fit(X_log, y_log)
y_log_pred = log_reg.predict(X_log_test)
print("logistic regression accuracy score: ", accuracy_score(y_log_test, y_log_pred))

X_tree, X_tree_test, y_tree, y_tree_test = train_test_split(X_data, y_data, test_size = 0.2)

#perform PCA again
pca = TruncatedSVD(n_components=35, n_iter=7)
pca.fit(X_tree)
X_tree_test = pca.fit_transform(X_tree_test)
X_tree = pca.fit_transform(X_tree)

#decision tree classifier
tree_model = DecisionTreeClassifier()
tree_model = tree_model.fit(X_tree, y_tree)
y_tree_pred = tree_model.predict(X_tree_test)
print("decision tree classifier accuracy score: ", accuracy_score(y_tree_test, y_tree_pred))

X_ran, X_ran_test, y_ran, y_ran_test = train_test_split(X_data, y_data, test_size = 0.2)

#normalize the data
scaler = MaxAbsScaler()
scaler.fit(X_ran)
X_ran = scaler.transform(X_ran)
X_ran_test = scaler.transform(X_ran_test)

#perform PCA again
pca = TruncatedSVD(n_components=100, n_iter=7)
pca.fit(X_ran)
X_ran_test = pca.fit_transform(X_ran_test)
X_ran = pca.fit_transform(X_ran)

#decision tree classifier
ran_model = RandomForestClassifier(n_estimators = 500)
ran_model = ran_model.fit(X_ran, y_ran)
y_ran_pred = ran_model.predict(X_ran_test)
print("random forest classifier accuracy score: ", accuracy_score(y_ran_test, y_ran_pred))

X_vec, X_vec_test, y_vec, y_vec_test = train_test_split(X_data, y_data, test_size = 0.2)

#perform PCA again
pca = TruncatedSVD(n_components=100, n_iter=7)
pca.fit(X_vec)
X_vec_test = pca.fit_transform(X_vec_test)
X_vec = pca.fit_transform(X_vec)

#decision tree classifier
vec_model = SVC()
vec_model = vec_model.fit(X_vec, y_vec)
y_vec_pred = vec_model.predict(X_vec_test)
print("support vector model accuracy score: ", accuracy_score(y_vec_test, y_vec_pred))

X_net, X_net_test, y_net, y_net_test = train_test_split(X_data, y_data, test_size = 0.2)

pca = TruncatedSVD(n_components=1000, n_iter=10)
pca.fit(X_net)
X_net_test = pca.fit_transform(X_net_test)
X_net = pca.fit_transform(X_net)

X_net = torch.FloatTensor(X_net)
X_net_test = torch.FloatTensor(X_net_test)
y_net = torch.LongTensor(y_net)
y_net_test = torch.LongTensor(y_net_test)

class Net(nn.Module):
    def __init__(self):
      super().__init__()
      self.fc1 = nn.Linear(in_features=1000, out_features=600)
      self.fc2 = nn.Linear(in_features=600, out_features=300)
      self.output = nn.Linear(in_features=300, out_features=2)
 
    def forward(self, x):
      x = F.relu(self.fc1(x))
      x = F.relu(self.fc2(x))
      x = self.output(x)
      return x
        
net = Net()

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

epochs = 100
loss_arr = []
for i in range(epochs):
   y_hat = net.forward(X_net)
   loss = criterion(y_hat, y_net)
   loss_arr.append(loss)
 
   if i % 10 == 0:
       print(f'Epoch: {i} Loss: {loss}')
 
   optimizer.zero_grad()
   loss.backward()
   optimizer.step()

print('Finished Training')

preds = []
with torch.no_grad():
   for val in X_net_test:
       y_hat = net.forward(val)
       preds.append(y_hat.argmax().item())

print("neural net accuracy score: ", accuracy_score(y_net_test, preds))