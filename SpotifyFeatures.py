# -*- coding: utf-8 -*-
"""Version 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hR8PKmpp-R3KCiZAZ1DZB5WKXcD2lNWp

# Preprocessing for Classifcation
"""

import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score


# One hot encode genre
# Basically spotify does duplicate songs with the same genre

filepath = "./df_hamadani_preprocessed_ALLSONGS.csv"
df = pd.read_csv(filepath)
df_final = df

df

#Data processing on the processing

#drop first (unnamed column) and trackid
df = df.drop(df.columns[0], axis=1)
df = df.drop(['track_id'], axis = 1)
df = df.drop(["Children’s Music"], axis = 1)


#popularity at the end
# columnsTitles = ["genre", "track_id", "valence",	"acousticness", "danceability",	"duration_ms", "energy", "instrumentalness", "key",	"liveness",	"loudness",	"mode",	"speechiness",	"tempo",	"time_signature","popularity"]
# df = df.reindex(columns = columnsTitles)

columnsTitles = ["acousticness", "danceability", "duration_ms", "energy",	"instrumentalness",	"key",	"liveness", "loudness",	"mode", "speechiness", "tempo",	"time_signature", "valence", "artist_popularity", "A Capella", "Indie",	"Ska",	"Rock",	"Hip-Hop",	"Classical",	"Folk",	"Soul",	"Jazz",	"Movie",	"R&B",	"Alternative",	"World",	"Dance",	"Reggaeton",	"Children's Music",	"Country", "Pop",	"Reggae",	"Comedy",	"Blues",	"Rap",	"Anime",	"Opera",	"Electronic",	"Soundtrack", "song_popularity"]
df = df.reindex(columns = columnsTitles)

df["mode"] = pd.factorize(df["mode"])[0]

#normalize data
x = df.values
x = x[:, :-1]
print(x.shape)
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df = pd.DataFrame(x_scaled)
df.columns = columnsTitles = ["acousticness", "danceability", "duration_ms", "energy",	"instrumentalness",	"key",	"liveness"," loudness",	"mode", "speechiness", "tempo",	"time_signature", "valence", "artist_popularity", "A Capella", "Indie",	"Ska",	"Rock",	"Hip-Hop",	"Classical",	"Folk",	"Soul",	"Jazz",	"Movie",	"R&B",	"Alternative",	"World",	"Dance",	"Reggaeton",	"Children's Music",	"Country", "Pop",	"Reggae",	"Comedy",	"Blues",	"Rap",	"Anime",	"Opera",	"Electronic",	"Soundtrack"]

df['song_popularity'] = df_final['song_popularity']

#convert popularity to binary 0 and 1 based on a threshold
threshold = 49
arr = df['song_popularity'].to_numpy()
print(arr)
for key, val in enumerate(arr):
  if val >= threshold:
    arr[key] = 1
  else:
    arr[key] = 0

df['song_popularity'] = arr

df

X_data = (df.loc[:, df.columns != 'song_popularity']).to_numpy()
y_data = (df.loc[:, 'song_popularity']).to_numpy()
print(X_data.shape)
print(y_data.shape)

X, X_test, y, y_test = train_test_split(X_data, y_data, test_size = 0.15, random_state = 42)
print('Training data:\nX shape = ',X.shape,'\ny shape = ',y.shape,'\n\nHeld out test data:\nX_test shape = ',X_test.shape,'\ny_test shape = ',y_test.shape,'\n')



"""# Binary Classification"""

log_reg = LogisticRegression(solver = "sag", C = 0.6)
log_reg.fit(X, y)

score = log_reg.score(X_test, y_test)
print(score)

from sklearn.metrics import precision_recall_fscore_support as score
from sklearn.metrics import f1_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
predicted = np
# for i in X_test:
#   print(i)
#   print(log_reg.predict(X_test))
#   break
#   predicted.append(log_reg.predict(i))

predicted = np.array(log_reg.predict(X_test))
print(y_test.shape)

# precision, recall, fscore, support = score(y_test, predicted)


print('mean squared error: {}'.format(mean_squared_error(y_test, predicted)))

tree_model = DecisionTreeClassifier(max_depth = 8)
tree_model.fit(X, y)

score = tree_model.score(X_test, y_test)
print(score)

predicted = np.array(tree_model.predict(X_test))
print(y_test.shape)

# precision, recall, fscore, support = score(y_test, predicted)


print('fscore: {}'.format(f1_score(y_test, predicted)))
print('mean squared error: {}'.format(mean_squared_error(y_test, predicted)))

df_final["song_popularity"].describe()

linear_reg = LinearRegression()
 linear_reg.fit(X, y)

score = linear_reg.score(X_test, y_test)
print(score)

predicted = np.array(linear_reg.predict(X_test))
print(y_test.shape)

# precision, recall, fscore, support = score(y_test, predicted)


print('mean squared error: {}'.format(mean_squared_error(y_test, predicted)))

from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors = 3, algorithm = 'kd_tree', weights = 'distance')
model.fit(X, y)

score = model.score(X_test, y_test)
print(score)

predicted = np.array(model.predict(X_test))
print(y_test.shape)

# precision, recall, fscore, support = score(y_test, predicted)


print('fscore: {}'.format(f1_score(y_test, predicted)))
print('mean squared error: {}'.format(mean_squared_error(y_test, predicted)))



X_train, X_val, y_train, y_val  = train_test_split(X, y, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2

#Decision Tree Regressor
import os
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        # Number of input features is 12.
        self.layer_1 = nn.Linear(40, 64) 
        self.layer_2 = nn.Linear(64, 128)
        self.layer_25 = nn.Linear(128, 128)
        self.layer_3 = nn.Linear(128, 64)
        self.layer_out = nn.Linear(64, 1) 
        
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.1)
        self.batchnorm1 = nn.BatchNorm1d(64)
        self.batchnorm2 = nn.BatchNorm1d(128)
        
    def forward(self, inputs):
        x = self.relu(self.layer_1(inputs))
        x = self.batchnorm1(x)
        x = self.relu(self.layer_2(x))
        x = self.batchnorm2(x)
        x = self.relu(self.layer_25(x))
        x = self.dropout(x)
        x = self.relu(self.layer_3(x))
        x = self.dropout(x)
        x = self.layer_out(x)
        
        return x

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print('Using {} device'.format(device))
model = NeuralNetwork().to(device)
print(model)

import torch.optim as optim
trainloader = torch.utils.data.DataLoader([X_val, y_val], batch_size=4,
                                          shuffle=True, num_workers=2)

from torch.utils.data import Dataset, DataLoader

class trainData(Dataset):
    def __init__(self, X_data, y_data):
        self.X_data = X_data
        self.y_data = y_data
        
    def __getitem__(self, index):
        return self.X_data[index], self.y_data[index]
        
    def __len__ (self):
        return len(self.X_data)

model = NeuralNetwork()
model.to(device)
print(model)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)


train_data = trainData(torch.FloatTensor(X), 
                       torch.FloatTensor(y))

train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)
def binary_acc(y_pred, y_test):
    y_pred_tag = torch.round(torch.sigmoid(y_pred))

    correct_results_sum = (y_pred_tag == y_test).sum().float()
    acc = correct_results_sum/y_test.shape[0]
    acc = torch.round(acc * 100)
    
    return acc

model.train()
for e in range(1, 15 + 1):
    epoch_loss = 0
    epoch_acc = 0
    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        
        y_pred = model(X_batch)
        
        loss = criterion(y_pred, y_batch.unsqueeze(1))
        acc = binary_acc(y_pred, y_batch.unsqueeze(1))
        
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
        epoch_acc += acc.item()
        

    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')

predict = []
predict.append(model(torch.tensor(X_test).float()))

predicted = np.array(predict)
# print(predicted.shape)
# print(predicted[0])
# print(y_test.shape)

# precision, recall, fscore, support = score(y_test, predicted)

print(y_test)
print(predicted[0].detach().numpy())

test = []
for i in predicted[0].detach().numpy():
  if i[0] > 0:
    test.append(1)
  else:
    test.append(0)

print('fscore: {}'.format(f1_score(torch.from_numpy(y_test), test)))
print('mean squared error: {}'.format(mean_squared_error(torch.from_numpy(y_test), test)))

"""# Preprocessing for Continuous Target Variable"""

# df
filepath = "./df_hamadani_preprocessed_ALLSONGS.csv"
df = pd.read_csv(filepath)
df_final = df

#Data processing on the processing

#drop first (unnamed column) and trackid
df = df.drop(df.columns[0], axis=1)
df = df.drop(['track_id'], axis = 1)
df = df.drop(["Children’s Music"], axis = 1)


#popularity at the end
# columnsTitles = ["genre", "track_id", "valence",	"acousticness", "danceability",	"duration_ms", "energy", "instrumentalness", "key",	"liveness",	"loudness",	"mode",	"speechiness",	"tempo",	"time_signature","popularity"]
# df = df.reindex(columns = columnsTitles)

columnsTitles = ["acousticness", "danceability", "duration_ms", "energy",	"instrumentalness",	"key",	"liveness", "loudness",	"mode", "speechiness", "tempo",	"time_signature", "valence", "artist_popularity", "A Capella", "Indie",	"Ska",	"Rock",	"Hip-Hop",	"Classical",	"Folk",	"Soul",	"Jazz",	"Movie",	"R&B",	"Alternative",	"World",	"Dance",	"Reggaeton",	"Children's Music",	"Country", "Pop",	"Reggae",	"Comedy",	"Blues",	"Rap",	"Anime",	"Opera",	"Electronic",	"Soundtrack", "song_popularity"]
df = df.reindex(columns = columnsTitles)

df["mode"] = pd.factorize(df["mode"])[0]

#normalize data
x = df.values
x = x[:, :-1]
print(x.shape)
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
df = pd.DataFrame(x_scaled)
df.columns = columnsTitles = ["acousticness", "danceability", "duration_ms", "energy",	"instrumentalness",	"key",	"liveness"," loudness",	"mode", "speechiness", "tempo",	"time_signature", "valence", "artist_popularity", "A Capella", "Indie",	"Ska",	"Rock",	"Hip-Hop",	"Classical",	"Folk",	"Soul",	"Jazz",	"Movie",	"R&B",	"Alternative",	"World",	"Dance",	"Reggaeton",	"Children's Music",	"Country", "Pop",	"Reggae",	"Comedy",	"Blues",	"Rap",	"Anime",	"Opera",	"Electronic",	"Soundtrack"]

df['song_popularity'] = df_final['song_popularity']

df

X_data = (df.loc[:, df.columns != 'song_popularity']).to_numpy()
y_data = (df.loc[:, 'song_popularity']).to_numpy()
print(X_data.shape)
print(y_data.shape)
X, X_test, y, y_test = train_test_split(X_data, y_data, test_size = 0.15, random_state = 42)
print('Training data:\nX shape = ',X.shape,'\ny shape = ',y.shape,'\n\nHeld out test data:\nX_test shape = ',X_test.shape,'\ny_test shape = ',y_test.shape,'\n')

"""# Regression on Continuous Target Variable"""

#Decision Tree Regressor
import os
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

regr = DecisionTreeRegressor(max_depth=10, splitter = 'best', ccp_alpha = 0.01)
regr.fit(X, y)
print(regr)
score = regr.score(X_test, y_test)
print(score)

predicted = np.array(regr.predict(X_test))
print(y_test.shape)

# precision, recall, fscore, support = score(y_test, predicted)


print('mean squared error: {}'.format(mean_squared_error(y_test, predicted)))
print('r2 score: {}'.format(r2_score(y_test, predicted)))

from sklearn.neighbors import KNeighborsRegressor
model = KNeighborsRegressor(n_neighbors = 9, leaf_size = 10)
model.fit(X, y)

score = model.score(X_test, y_test)
print(score)

predicted = np.array(model.predict(X_test))
print(y_test.shape)

# precision, recall, fscore, support = score(y_test, predicted)


print('mean squared error: {}'.format(mean_squared_error(y_test, predicted)))

print('mean squared error: {}'.format(mean_squared_error(y_test, predicted)))
print('r2 score: {}'.format(r2_score(y_test, predicted)))

X_train, X_val, y_train, y_val  = train_test_split(X, y, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2

import os
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        # Number of input features is 12.
        self.layer_1 = nn.Linear(40, 64) 
        self.layer_2 = nn.Linear(64, 128)
        self.layer_25 = nn.Linear(128, 128)
        self.layer_3 = nn.Linear(128, 64)
        self.layer_out = nn.Linear(64, 1) 
        
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.1)
        self.batchnorm1 = nn.BatchNorm1d(64)
        self.batchnorm2 = nn.BatchNorm1d(128)
        
    def forward(self, inputs):
        x = self.relu(self.layer_1(inputs))
        x = self.batchnorm1(x)
        x = self.relu(self.layer_2(x))
        x = self.batchnorm2(x)
        x = self.relu(self.layer_25(x))
        x = self.dropout(x)
        x = self.relu(self.layer_3(x))
        x = self.dropout(x)
        x = self.layer_out(x)
        
        return x

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print('Using {} device'.format(device))
model = NeuralNetwork().to(device)
print(model)

import torch.optim as optim
trainloader = torch.utils.data.DataLoader([X_val, y_val], batch_size=4,
                                          shuffle=True, num_workers=2)

from torch.utils.data import Dataset, DataLoader

class trainData(Dataset):
    def __init__(self, X_data, y_data):
        self.X_data = X_data
        self.y_data = y_data
        
    def __getitem__(self, index):
        return self.X_data[index], self.y_data[index]
        
    def __len__ (self):
        return len(self.X_data)

model = NeuralNetwork()
model.to(device)
print(model)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)


train_data = trainData(torch.FloatTensor(X_train), 
                       torch.FloatTensor(y_train))

train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)
# def binary_acc(y_pred, y_test):
#     y_pred_tag = torch.round(torch.sigmoid(y_pred))

#     correct_results_sum = (y_pred_tag == y_test).sum().float()
#     acc = correct_results_sum/y_test.shape[0]
#     acc = torch.round(acc * 100)
    
#     return acc

print(len(train_loader))
print(y)
model.train()
for e in range(1, 15 + 1):
    epoch_loss = 0
    epoch_acc = 0
    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        
        y_pred = model(X_batch)
        
        loss = criterion(y_pred, y_batch.unsqueeze(1))
        # acc = binary_acc(y_pred, y_batch.unsqueeze(1))
        
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
        epoch_acc += acc.item()
        

    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')

predict = []
predict.append(model(torch.tensor(X_test).float()))

predicted = np.array(predict)
# print(predicted.shape)
# print(predicted[0])
# print(y_test.shape)

# precision, recall, fscore, support = score(y_test, predicted)

print(y_test)
print(predicted[0].detach().numpy())

print('mean squared error: {}'.format(mean_squared_error(torch.from_numpy(y_test), predicted[0].detach().numpy())))
print('r2 score: {}'.format(r2_score(torch.from_numpy(y_test), predicted[0].detach().numpy())))

import pandas as pd
import seaborn as sn
import matplotlib.pyplot as plt

from matplotlib.pyplot import figure

figure(figsize=(14, 12), dpi=80)

corrMatrix = df.corr()
sn.heatmap(corrMatrix, annot=True)
plt.show()

