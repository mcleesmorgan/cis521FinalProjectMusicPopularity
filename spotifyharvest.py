# -*- coding: utf-8 -*-
"""SpotifyHarvest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1go7fh9Jt0qnG9qCMQhBnApqAETBUo8zo

**Note:** Many of the datasets come from kaggle, so naming them after the person who created the datasets so we can distinguish between them
"""

import pandas as pd

"""### Hamidani data

This is the dataset that Morgan found here: https://www.kaggle.com/zaheenhamidani/ultimate-spotify-tracks-db
"""

df_hamidani = pd.read_csv("/content/hamidani_data.csv")
df_hamidani

"""### Yamac data

Another Kaggle dataset, 160k+ tracks from 1921-2020: https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks/activity

- This contains multiple csvs, but they are all different aggregations/groupings of the master file, titled "data.csv"

"""

df_yamac = pd.read_csv("/content/yamac_data.csv")

df_yamac

"""# Figueroa Data

- 1.2M+ songs ????
- https://www.kaggle.com/rodolfofigueroa/spotify-12m-songs
- Confirmed: Some psycho/genius scraped 1.2M+ songs, but its missing song and artist popularity (and less importantly, genre)
- Since this is still so much larger than all the other datasets I'm finding combined, it might be worth using this and scraping artist and song popularity ourselves
"""

df_figueroa = pd.read_csv("/content/figueroa.csv")
# df_figueroa = pd.read_csv("/figueroa.csv")

df_figueroa



# Are there songs in yamac and hamidani not in figueroa? Seeing if it would be worth keeping them

hamidani_songs = list(set(list(df_hamidani['track_id'])))

yamac_songs = list(set(list(df_yamac['id'])))

figueroa_songs = list(set(list(df_figueroa['id'])))

# 202,044 hamadani tracks NOT in figueroa. 
# So most do not overlap
df_hamidani[~df_hamidani['track_id'].isin(figueroa_songs)]

# 153,206 songs in Yamac not in figueroa
# Again not a lot of overlap

df_yamac[~df_yamac['id'].isin(figueroa_songs)]

# Overlap between yamac and hamidani

# Again minimal
df_yamac[~df_yamac['id'].isin(hamidani_songs)]

df_figueroa.columns

df_hamidani.columns

df_yamac.columns

for col in df_hamidani.columns:
  if col not in df_figueroa.columns:
    print(col)

for col in df_figueroa.columns:
  if col not in df_hamidani.columns:
    print(col)

"""The two largest datasets by a lot are Figueroa (1.2M+) and Hamadani (230k+) 

**Figueroa vs. Hamadani**
- Hamadani has genre
- Hamadani has song popularity, Figueroa does not (we'd need to scrape)
- Figueroa has year, release date, explicit boolean, and waaaay more songs


**Neither have artist popularity, so we need to scrape this either way**


# Thoughts/Next steps
- Since I need to scrape artist popularity either way, do this next to get an idea of how long it takes to pull from the Spotify API. Use this info to guage whether its worth including the Figueroa dataset since we would have to scrape song popularity (and should probably do genre too) for 1 million songs
"""



"""### Other possible datasets - Havent looked at them in-depth
- tomigelo - 130k songs: https://www.kaggle.com/tomigelo/spotify-audio-features 
- Nakhaee - 18k songs with track popularity and lyrics, no artist popularity: https://www.kaggle.com/imuhammad/audio-features-and-lyrics-of-spotify-songs
- Carbone - Songs by decade with song popularity, high kaggle usability rating: https://www.kaggle.com/cnic92/spotify-past-decades-songs-50s10s
- Ansari - 40k+ songs, popularity missing: https://www.kaggle.com/theoverman/the-spotify-hit-predictor-dataset?select=dataset-of-00s.csv

# Testing how long it takes to scrape artist popularity from the Hamadani dataset
- This will allow me to guage whether we can scrape for the Figueroa dataset (song popularity, artist popularity, and genre-if we want).
"""

pip install spotipy

import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
cid = '7ee8f9f0fd034935a9abdb6762ded237'
secret = '85a312f4836242098490468bd781c57f'
client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret)
sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)

import sys
from pprint import pprint


# Show info for one artist - Weezer - by using artist id
urn = 'spotify:artist:3jOstUTkEu2JkjvRdBA5Gu'
urn = 'spotify:artist:5t28BP42x2axFnqOOMg3CM'


artist = sp.artist(urn)
pprint(artist)

'''
    usage: show_tracks.py path_of_ids
    given a list of track IDs show the artist and track name
'''
max_tracks_per_call = 50

# List of track ids
tids = ['2ziWXUmQLrXTiYjCg2fZ2t', '1wsRitfRRtWyEapl0q22o8', '0BRjO6ga9RKCKjfDqeFgWV', '7zyJ2g9o0nVOBbDavVveCl'] # still need to add


for start in range(0, len(tids), max_tracks_per_call):
  results = sp.tracks(tids[start: start + max_tracks_per_call])
  for track in results['tracks']:
    print(track['name'] + ' - ' + track['artists'][0]['name'] + ":    " + str(track['popularity']))

"""# Scrape Hamadani artist popularity
- And write it to a csv
"""

# unique list of artists from Hamadani dataset
artists_hamadani = list(set(df_hamidani.artist_name))
len(artists_hamadani) # 14,564 unique artists

# for artist in artists_hamadani[0:5]:



results = sp.search(q=artist, type='artist', limit=1)
results

dict_artist_pop = {} # dictionary of artists and their popularity
error_dict = {} # The key is the name we searched for, value is the name returned which does NOT match search
notfound_list = [] # Names of users where the search query produced 0 results

for artist in artists_hamadani: # CHANGE THIS BACK TO FULL LIST AFTER FINISHING RUN

  print("Searching for:", artist)


  results = sp.search(q=artist, type='artist', limit=1)

  # if search result returned nothing
  if len(results['artists']['items']) == 0:
    notfound_list.append(artist) # append this name to the list

  # The search found something
  else:

    artist_pop = results['artists']['items'][0]['popularity']
    artist_name = results['artists']['items'][0]['name']
    # print("Artist name: ", artist_name, ", Artist popularity:", artist_pop)

    # If the name being searched matches the artist name exactly
    if artist.lower() == artist_name.lower():
      # add to the dictionary
      dict_artist_pop[artist_name] = artist_pop

    else: 
      print("ERROR: Searched for ", artist, " but found ", artist_name)
      error_dict[artist] = artist_name
    

# dict_artist_pop

len(error_dict) # 584
len(dict_artist_pop) # 13960
len(notfound_list) # 18

dict_artist_pop
error_dict

# For all of the artists that we found an exact match using Spotify's search api, write their popularity to a csv
df_artist_pop = pd.DataFrame.from_dict(dict_artist_pop, orient='index')
df_artist_pop.to_csv('/content/hamidani_artist_pop_data.csv')

df_artist_pop.describe()

"""# Add popularity to Hamadani dataset"""

df_artist_pop.columns = ['artist_popularity']

# Merge two dfs
df_hamadani_w_pop = df_hamidani.merge(df_artist_pop, how='inner', left_on='artist_name', right_index=True)

# Change column popularity to song popularity
df_hamadani_w_pop.rename(columns={"popularity": "song_popularity"}, inplace=True)

# Show rows with duplicate songs
# df_hamadani_w_pop.drop_duplicates(subset=['track_id'])
df_hamadani_w_pop[df_hamadani_w_pop.duplicated(subset='track_id')].sort_values(by=['track_id']).head(60)

df_hamadani_w_pop['genre'].value_counts()

# Write dataframe to CSV
df_hamadani_w_pop.to_csv('/content/df_hamadani_w_pop.csv')

"""## Notes on file I created - df_hamadani_w_pop.csv
- This consists of the data from Morgan's dataset for which I could identify the artist in the API and get the corresponding artist popularity
- The dataset has a relatively even mix of ~9000 per genre (for most genres)
- Most of it looks good but there are some incorrect genres (Children's music for a 5 Finger Death Punch song)
- **There are duplicate songs in the df because one song may be assigned multiple genres**. So we either:
	- One hot encode the genres and  add them as new columns
		- Maybe can do this by using pd.get_dummies() on the genre column, groupby track on the result using .sum()
	- Drop the genres column and  remove duplicate track rows from the df
- **There are 225,510 rows but after dropping duplicate tracks there are 171, 041**

# Scrape Figueroa meta
- We need artist popularity, song popularity, and (if possible) genre

## First scrape artist popularity
- This will tell me how long it will take to scrape song and genre as well
"""

from ast import literal_eval
# Convert all of the lists values in artist_ids to an actual list
df_figueroa['artist_ids'] = df_figueroa['artist_ids'].apply(literal_eval)
df_figueroa

# explode artist_ids
df_fig_explode = df_figueroa.explode('artist_ids')
df_fig_explode


# use df_fig_explode to get unique list of artist_ids
fig_artist_ids = list(set(df_fig_explode['artist_ids']))
fig_artist_ids

###################
# SCRAPE ARTIST POPULARITY AND GENRES
###################


# Create dictionaries to store mappings
dict_artist_pop = {} # dictionary of artists ids and their artist popularity
dict_artist_genre = {} # dictionary of artist ids and their genre
count = 0

for artist in fig_artist_ids:

  # print("Searching for:", artist)

  ###################
  urn = 'spotify:artist:' + artist
  result = sp.artist(urn)

  artist_pop = result['popularity']
  artist_name = result['name']
  artist_genres = result['genres']

  # Add to the dictionaries
  dict_artist_pop[artist] = artist_pop
  dict_artist_genre[artist] = artist_genres

  # print("Artist name: ", artist_name, ", Artist popularity:", artist_pop)
  count += 1
  

  if count % 1000 == 0:
    print(count, " songs scraped")
  # pprint(result["genres"])

df_artist_pop = pd.DataFrame.from_dict(dict_artist_pop, orient='index')
df_artist_pop.to_csv('/content/drive/MyDrive/Figueroa_artist_pop_data.csv')

df_artist_genre = pd.DataFrame.from_dict(dict_artist_genre, orient='index')
df_artist_genre.to_csv('/content/drive/MyDrive/Figueroa_artist_pop_data.csv')
df_artist_genre

from google.colab import drive
drive.mount('/content/drive')

dict_artist_genre

"""# Final Pre-Processing
Taking df_hamadani_w_pop.csv and dropping any songs for which we do not have lyric data for using Morgan's list

- Creating one version where genres are one-hot encoded
- Creating another version where genres and duplicate rows are removed
"""

# manually uploading both files

df_tracks_w_lyrics = pd.read_csv("/content/lyrics_track_names.csv")
df_hamadani_w_pop = pd.read_csv("/content/df_hamadani_w_pop.csv")

df_hamadani_w_pop

# drop songs appearing as duplicates
df_tracks_w_lyrics.drop_duplicates(subset=['track_name'], inplace=True)

# Create df consisting of unique list of songs
tracks_with_lyrics = df_tracks_w_lyrics[['track_name']]
tracks_with_lyrics

# Drop songs from audio dataset where we do not have their lyrics
df_hamadani_w_lyrics = df_hamadani_w_pop.merge(tracks_with_lyrics, how='inner', left_on='track_name', right_on='track_name')

# This leaves 20,830 rows but only 6,337 unique songs
# Discrepancy is from songs appearing multiple times when they are assigned multiple genres
# and when multiple artists cover the same song
df_hamadani_w_lyrics

# Unique genres in dataset
# Note: in the original dataset theres ~10k songs per genre. The distribution is no longer even
# I wonder how many of these songs do not contain lyrics in their music--rather than are not on the Genius API
df_hamadani_w_lyrics.value_counts(subset=['genre'])

import copy

# genre_list = list(set(df_hamadani_onehot['genre']))
genre_list = list(set(df_hamadani_w_pop['genre']))

df_hamadani_onehot = copy.deepcopy(df_hamadani_w_lyrics) 

for genre in genre_list:

  df_hamadani_onehot[genre] = df_hamadani_onehot['genre'].apply(lambda x: 1 if x==genre else 0)

# df_hamadani_w_lyrics['genre'].apply(lambda x: 1 if x == 'Alternative' else 0)

df_hamadani_onehot.value_counts(subset=['track_id'])

# Test a song with 8 genres assigned
df_hamadani_onehot[df_hamadani_onehot['track_id'] == '6sVQNUvcVFTXvlk3ec0ngd']

# we will only need to genre columns from this df
df_onehot_groupby = df_hamadani_onehot.groupby(by='track_id').sum()
# drop everything except genre columns
df_onehot_groupby = df_onehot_groupby[genre_list]
df_onehot_groupby

# drop all the genre columns currently in df_hamadani_onehot
df_hamadani_onehot.drop(columns=genre_list, inplace=True)

# merge the onehot genre columns back onto this (this difference with this new version is one row can contain multiple genres marked as 1)
df_hamadani_onehot = df_hamadani_onehot.merge(df_onehot_groupby, how='left', left_on='track_id', right_index=True)


# Now drop rows with duplicate track ids
df_hamadani_onehot.drop_duplicates(subset=['track_id'], inplace=True)

df_hamadani_onehot

# Testing
##########################################



df_hamadani_onehot['track_name'].drop_duplicates() # 6332 track names
df_hamadani_onehot['track_id'].drop_duplicates() # 10870 track_ids

# Some songs have very common track names that appear over and over
df_hamadani_onehot.groupby(by=['track_name']).count().sort_values(by=['track_id'], ascending=False).head(60)

# Testing

df_hamadani_onehot[df_hamadani_onehot['track_name'] == 'You']

# Test a song with 8 genres assigned
df_hamadani_onehot[df_hamadani_onehot['track_id'] == '6sVQNUvcVFTXvlk3ec0ngd']

# Test another song with 7 genres assigned
df_hamadani_onehot[df_hamadani_onehot['track_id'] == '7Kho44itYaCQZvZQVV2SLW']

# Drop columns we dont need FOR SURE
df_hamadani_onehot.drop(columns=['Unnamed: 0', 'genre', 'track_name', 'artist_name'], inplace=True)

df_hamadani_onehot

df_hamadani_onehot.key.value_counts()

df

# doing ordinal encoding of key

# create dictionary of encodings for key
key_dict = {"A": 1,
            "A#": 2,
            "B": 3,
            "C": 4,
            "C#": 5,
            "D": 6,
            "D#": 7,
            "E": 8,
            "F": 9,
            "F#": 10,
            "G": 11,
            "G#": 12,
}

# replace key strings with ints
df_hamadani_onehot['key'] = df_hamadani_onehot['key'].apply(lambda x: key_dict[x])

df_hamadani_onehot

# Encode mode (major or minor) with 1 or 0
df_hamadani_onehot['mode'] = df_hamadani_onehot['mode'].apply(lambda x: 1 if x=='Major' else 0)

df_hamadani_onehot

# Encode time signatures with ints
time_signature_dict = {'1/4': 1,
                       '3/4': 2,
                       '4/4': 3,
                       '5/4': 4}

df_hamadani_onehot['time_signature'] = df_hamadani_onehot['time_signature'].apply(lambda x: time_signature_dict[x])

df_hamadani_onehot

"""I think at this point the df_hamadani_onehot is ready for ML. The only thing we would need to do is drop 'track_id' and possibly drop the genres.

Previous models overfitted when too many features were used, so for this reason we may want to drop genre

"""

df_hamadani_onehot.to_csv('/content/df_hamadani_preprocessed.csv')