# -*- coding: utf-8 -*-
"""LinRegEnsemble.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ew3rJB03QSsqP-feOAkPpeYzcD1wJD-Z
"""

import pandas as pd
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import math
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import ElasticNet
from matplotlib.pyplot import plot
import matplotlib.pyplot as plt
from matplotlib.pyplot import scatter
from sklearn.decomposition import PCA
import nltk.tokenize
from nltk.probability import FreqDist
nltk.download("punkt")

"""# Audio Features

### With genre features, no regularization
"""

# Manually uploading df_hamadani_preprocessed.csv

df_audio = pd.read_csv("/content/df_hamadani_preprocessed.csv")

# Drop 2 columns - track_id and unnamed:0
df_audio.drop(columns=['track_id', 'Unnamed: 0'], inplace=True)

df_audio

# Create one df of samples and another of the labels
X = df_audio.drop(columns=["song_popularity"])
y = df_audio[['song_popularity']]

# split into training and test_sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create linear regression object
linreg = linear_model.LinearRegression()

# Fit the model
linreg.fit(X_train, y_train)

# Made predictions
y_pred = linreg.predict(X_test)

# MSE and RMSE
print("Model w/out regularization and with Genre features\n")
print("MSE:", mean_squared_error(y_test, y_pred))
print("RMSE:", math.sqrt(mean_squared_error(y_test, y_pred)))
print("R2 score:", linreg.score(X_test, y_test))

plt.scatter(y_test, y_pred, s=10)
plt.ylabel("Predictions")
plt.xlabel("Observations")
plt.xlim(-1,100)
plt.ylim(-1,100)

"""### With genre features, L1 regularization

This performs slightly worse than model without regularization
"""

# Testing several different alphas
for alpha_val in [0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1, 1.5, 2, 3, 4, 5]:
                  
  # Create linear regression object
  linreg = linear_model.Lasso(alpha=alpha_val)

  # Standardize the data
  scaler = StandardScaler()
  X_train_scaled = scaler.fit_transform(X_train)
  X_test_scaled = scaler.transform(X_test)

  # Fit the model
  linreg.fit(X_train_scaled, y_train)

  # Made predictions
  y_pred = linreg.predict(X_test_scaled)


  # MSE and RMSE
  print("\n\nALPHA =", alpha_val)
  print("Model w/out regularization and with Genre features")
  print("MSE:", mean_squared_error(y_test, y_pred))
  print("RMSE:", math.sqrt(mean_squared_error(y_test, y_pred)))
  print("R2 score:", linreg.score(X_test_scaled, y_test))

"""
### With genre features, L2 regularization

L2 regularization performs slightly **worse** than without regularization -- measured by both RMSE and R2 score
"""

# Testing several different alphas
for alpha_val in [0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1, 1.5, 2, 3, 4, 5]:
                  
  # Create linear regression object
  linreg = linear_model.Ridge(alpha=alpha_val)

  # Standardize the data
  scaler = StandardScaler()
  X_train_scaled = scaler.fit_transform(X_train)
  X_test_scaled = scaler.transform(X_test)

  # Fit the model
  linreg.fit(X_train_scaled, y_train)

  # Made predictions
  y_pred = linreg.predict(X_test_scaled)


  # MSE and RMSE
  print("\n\nALPHA =", alpha_val)
  print("Model w/out regularization and with Genre features")
  print("MSE:", mean_squared_error(y_test, y_pred))
  print("RMSE:", math.sqrt(mean_squared_error(y_test, y_pred)))
  print("R2 score:", linreg.score(X_test_scaled, y_test))

"""## W/o Genre, no regularization
- This is much worse after dropping genre. I also tried doing L1 and L2 regularization but it was similarly bad
"""

# Drop genre columns
X_no_genre = X[X.columns[0:14]]

# split into training and test_sets
X_train, X_test, y_train, y_test = train_test_split(X_no_genre, y, test_size=0.2, random_state=42)

# Create linear regression object
linreg = linear_model.LinearRegression()

# Fit the model
linreg.fit(X_train, y_train)

# Made predictions
y_pred = linreg.predict(X_test)

# MSE and RMSE
print("Model w/out regularization and with Genre features\n")
print("MSE:", mean_squared_error(y_test, y_pred))
print("RMSE:", math.sqrt(mean_squared_error(y_test, y_pred)))
print("R2 score:", linreg.score(X_test, y_test))

"""## Dropping artist popularity
The previous literature would suggest that artist popularity is giving us a big boost to our predictive power. What if I take our best model (Vanilla LinReg w/ genre features included) and change it by ONLY removing artist popularity

**Answer:** It makes it worse, but surprisingly not by much
"""

# Change normal process by dropping artist popularity as well
X = df_audio.drop(columns=["song_popularity", 'artist_popularity'])
y = df_audio[['song_popularity']]


# split into training and test_sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create linear regression object
linreg = linear_model.LinearRegression()

# Fit the model
linreg.fit(X_train, y_train)

# Made predictions
y_pred = linreg.predict(X_test)


# MSE and RMSE
print("Model w/out regularization and with Genre features\n")
print("MSE:", mean_squared_error(y_test, y_pred))
print("RMSE:", math.sqrt(mean_squared_error(y_test, y_pred)))
print("R2 score:", linreg.score(X_test, y_test))

"""## LinReg w/ PCA
- PCA just barely outperforms the 2nd best model
- I tested several values for n_components and n=39 is the best. Note there are 41 features
"""

X = df_audio.drop(columns=["song_popularity"])
y = df_audio[['song_popularity']]

# split into training and test_sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Standardize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# Apply PCA
# pca = PCA(n_components='mle', svd_solver='full')
pca = PCA(n_components=39)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)


# Create linear regression object
linreg = linear_model.LinearRegression()

# Fit the Lin Reg model
linreg.fit(X_train_pca, y_train)

# Made predictions
y_pred = linreg.predict(X_test_pca)


# MSE and RMSE
print("Model w/out regularization and with Genre features\n")
print("MSE:", mean_squared_error(y_test, y_pred))
print("RMSE:", math.sqrt(mean_squared_error(y_test, y_pred)))
print("R2 score:", linreg.score(X_test_pca, y_test))

mae = (df_error['abs_error'].abs().sum())/len(df_error)

print("Mean Absolute Error of PCA model:", mae)

"""# Lyrics Classifier"""

#preprocess the lyrics and count the frequencies

corpus = []
y_data = []
y_lin_data = []
names = []
fileNames = ["SpotifyFeaturesLyricsEdited0.csv", "SpotifyFeaturesLyricsEdited1.csv", "SpotifyFeaturesLyricsEdited2.csv",\
             "SpotifyFeaturesLyricsEdited3.csv", "SpotifyFeaturesLyricsEdited4.csv", "SpotifyFeaturesLyricsEdited5.csv",\
             "SpotifyFeaturesLyricsEdited5.csv", "SpotifyFeaturesLyricsEdited6.csv", "SpotifyFeaturesLyricsEdited7.csv",\
             "SpotifyFeaturesLyricsEdited8.csv", "SpotifyFeaturesLyricsEdited9.csv", "SpotifyFeaturesLyricsEdited10.csv",\
             "SpotifyFeaturesLyricsEdited11.csv", "SpotifyFeaturesLyricsEdited12.csv", "SpotifyFeaturesLyricsEdited13.csv",\
             "SpotifyFeaturesLyricsEdited14.csv", "SpotifyFeaturesLyricsEdited15.csv"]
# "SpotifyFeaturesLyricsEdited16.csv",\
# "SpotifyFeaturesLyricsEdited17.csv"

total = 0
for name in fileNames:

  filepath = "/content/drive/MyDrive/clean/" + name

  # Read a single csv
  df = pd.read_csv(filepath)

  # for each row
  for (index, row) in df.iterrows():
    
    # tokenize lyrics into words and lower it
    lyr = nltk.word_tokenize(row['lyrics'])
    lyr = [l.lower() for l in lyr if l.isalpha()]
    
    
    y_data.append(row['popularity'])
    y_lin_data.append(row['popularity'])
    str1 = " "
    lyr = str1.join(lyr)
    corpus.append(lyr)
    names.append(row['track_name'])
  # print(df['popularity'].mean()) # print average song popularity for the file
  total = total + df['popularity'].mean()

threshold = 55
for i in range(len(y_data)):
  if y_data[i] >= threshold:
    y_data[i] = 1
  else:
    y_data[i] = 0

vectorizer = CountVectorizer(lowercase=True, strip_accents="ascii")
X_data = vectorizer.fit_transform(corpus)
# print("Avg: ", total / len(fileNames))
names = {"track_name": names}
namesDf = pd.DataFrame(data=names)

X_lin, X_lin_test, y_lin, y_lin_test = train_test_split(X_data, y_lin_data, test_size = 0.2, random_state=42)

# #normalize the data
# scaler = MaxAbsScaler()
# scaler.fit(X_lin)
# X_lin = scaler.transform(X_lin)
# X_lin_test = scaler.transform(X_lin_test)

# #perform PCA first
# pca = TruncatedSVD(n_components=500, n_iter=10)
# pca.fit(X_lin)
# X_lin = pca.fit_transform(X_lin)
# X_lin_test = pca.fit_transform(X_lin_test)

#logistic regression
lin_reg = ElasticNet(alpha=.5, l1_ratio = 0.4)
lin_reg = lin_reg.fit(X_lin, y_lin)
y_lin_pred = lin_reg.predict(X_lin_test)
print(lin_reg.score(X_lin_test, y_lin_test))
print(mean_squared_error(y_lin_test, y_lin_pred))
for i in range(len(y_lin_test)):
  print(y_lin_test[i], y_lin_pred[i])

len(y_lin_pred)

"""# Notes & Next steps
- The best model is PCA LinReg WITH genre features and W/O any regularization
- 2nd best model is same as above but without PCA. 
- Previous literature suggested we would get a big boost from using a social influence feature like artist popularity. And while we did get a boost, its less than I expected
- Adding regularization hurts perforamnce across the board
- Models perform much worse when genre features are dropped. One of the previous studies said they suffered from overfitting when they had too many features, but for us-- more features was better
- Maybe add CrossVal?
- Test bias and variance?
- My data has ~10k samples. I think Morgan's only has 6k so I dont know where that discrepancy is coming from.
  - If the missing 4k are just songs without lyrics, I could build the ensemble to only average them if lyrics are present
- Try with larger dataset. The distrubution of genres is kinda off any might be giving overconfidence
"""

