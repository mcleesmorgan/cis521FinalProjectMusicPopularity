# -*- coding: utf-8 -*-
"""LinRegEnsemble.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nI-0bOJrq3YEUi6FD6gGJ0Pzhk-CCrV5

# Description
In this Colab we planned to build an ensemble but later on decided to abandon the task due to technical difficulties. 

The idea was to have an ensemble with 2 Linear Regression learners. One learner would predict song popularity based on the Audio features dataset. The other would predict based on the Lyrics dataset. The final output would be an average of their predictions.

The main difficulty we faced was in getting the corresponding songs to match up between the two datasets so that we could average their predictions. Unlike the audio dataset which had a unique identifier (track_id) the lyrics dataset did not (track name was close, but there can be many songs with the same name). To match up songs between datasets, we considered rows a match when track_name AND song popularity matched. Given the Audio dataset was much larger than the Lyrics, we dropped everything from the Audio that was not a match in Lyrics. This matching heuristic was imperfect and our full dataset was reduced to ~ 6000 samples.

Unfortunately this was not enough. Due to the size of the feature space in the Lyrics dataset and limits imposed by Colab, when creating the dataset we had to 1) split the raw text data across several files and 2) turn the training/test sets into a sparse matrix once we vectorized the text. As a result we did not have a straightforward way to match up the predictions. 

In the end, due to the small number of samples remaining after using our matching heuristic, the amount of time it takes to scrape more data from the Genius API, and our current process of using a sparse matrix, we decided to abandon this task and focus our efforts on other models.
"""

import pandas as pd
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import math
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import ElasticNet
from matplotlib.pyplot import plot
import matplotlib.pyplot as plt
from matplotlib.pyplot import scatter
from sklearn.decomposition import PCA
import nltk.tokenize
from nltk.probability import FreqDist
nltk.download("punkt")

# Manually uploading df_hamadani_preprocessed.csv

# df_audio = pd.read_csv("/content/df_hamadani_preprocessed.csv")

# TESTING VERSION 2 - In this version, I dropped duplicate track names except for the first instances. Seeing if this creates a list that matches Morgans

df_audio = pd.read_csv("/content/df_hamadani_preprocessed2.csv")
df_audio

# Import list of Lyric track_names/popularity 
df_lyric_list = pd.read_csv('/content/lyrics_track_names_popularity.csv')

# Drop rows where track_name AND popularity in combination are duplicates
df_lyric_list.drop_duplicates(subset=['track_name', 'popularity'], inplace=True) # 6759 unique track_name/popularity pairs

df_lyric_list

# Do they match?
df_compare = df_audio[['track_name', 'song_popularity', 'track_id']].merge(df_lyric_list[['track_name', 'popularity']], how='inner', left_on='track_name', right_on='track_name')

# Only keep these track_ids
df_keep = df_compare[df_compare['song_popularity'] == df_compare['popularity']]
df_keep

# Drop all rows in df_audio where track_id is not in df_keep
df_audio = df_audio.merge(df_keep['track_id'], how='inner', left_on='track_id', right_on='track_id')
df_audio

# Drop 3 columns - track_id, track_name and unnamed:0
df_audio.drop(columns=['track_id', 'Unnamed: 0', 'track_name'], inplace=True)
df_audio

# Create one df of samples and another of the labels
X = df_audio.drop(columns=["song_popularity"])
y = df_audio[['song_popularity']]

# split into training and test_sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("# of training samples:",len(X_train))
print("# of test samples",len(X_test))

"""# Audio Features

### With genre features, no regularization
"""

# Create linear regression object
linreg = linear_model.LinearRegression()

# Fit the model
linreg.fit(X_train, y_train)

# Made predictions
y_pred = linreg.predict(X_test)

# MSE and RMSE
print("Model w/out regularization and with Genre features\n")
print("MSE:", mean_squared_error(y_test, y_pred))
print("RMSE:", math.sqrt(mean_squared_error(y_test, y_pred)))
print("R2 score:", linreg.score(X_test, y_test))

plt.scatter(y_test, y_pred, s=10)
plt.ylabel("Predictions")
plt.xlabel("Observations")
plt.xlim(-1,100)
plt.ylim(-1,100)

"""### With genre features, L1 regularization

This performs slightly worse than model without regularization for all alphas tested
"""

# Testing several different alphas
for alpha_val in [0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1, 1.5, 2, 3, 4, 5]:
                  
  # Create linear regression object
  linreg = linear_model.Lasso(alpha=alpha_val)

  # Standardize the data
  scaler = StandardScaler()
  X_train_scaled = scaler.fit_transform(X_train)
  X_test_scaled = scaler.transform(X_test)

  # Fit the model
  linreg.fit(X_train_scaled, y_train)

  # Made predictions
  y_pred = linreg.predict(X_test_scaled)


  # MSE and RMSE
  print("\n\nALPHA =", alpha_val)
  print("Model w/out regularization and with Genre features")
  print("MSE:", mean_squared_error(y_test, y_pred))
  print("RMSE:", math.sqrt(mean_squared_error(y_test, y_pred)))
  print("R2 score:", linreg.score(X_test_scaled, y_test))

"""
### With genre features, L2 regularization

L2 regularization performs slightly **worse** than without regularization -- measured by both RMSE and R2 score
"""

# Testing several different alphas
for alpha_val in [0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1, 1.5, 2, 3, 4, 5]:
                  
  # Create linear regression object
  linreg = linear_model.Ridge(alpha=alpha_val)

  # Standardize the data
  scaler = StandardScaler()
  X_train_scaled = scaler.fit_transform(X_train)
  X_test_scaled = scaler.transform(X_test)

  # Fit the model
  linreg.fit(X_train_scaled, y_train)

  # Made predictions
  y_pred = linreg.predict(X_test_scaled)


  # MSE and RMSE
  print("\n\nALPHA =", alpha_val)
  print("Model w/out regularization and with Genre features")
  print("MSE:", mean_squared_error(y_test, y_pred))
  print("RMSE:", math.sqrt(mean_squared_error(y_test, y_pred)))
  print("R2 score:", linreg.score(X_test_scaled, y_test))

"""## W/o Genre, no regularization
- This is much worse after dropping genre. I also tried doing L1 and L2 regularization but it was similarly bad
"""

# Drop genre columns
X_no_genre = X[X.columns[0:14]]

# split into training and test_sets
X_train, X_test, y_train, y_test = train_test_split(X_no_genre, y, test_size=0.2, random_state=42)

# Create linear regression object
linreg = linear_model.LinearRegression()

# Fit the model
linreg.fit(X_train, y_train)

# Made predictions
y_pred = linreg.predict(X_test)

# MSE and RMSE
print("Model w/out regularization and with Genre features\n")
print("MSE:", mean_squared_error(y_test, y_pred))
print("RMSE:", math.sqrt(mean_squared_error(y_test, y_pred)))
print("R2 score:", linreg.score(X_test, y_test))

"""## Dropping artist popularity
The previous literature would suggest that artist popularity is giving us a big boost to our predictive power. What if I take our best model (Vanilla LinReg w/ genre features included) and change it by ONLY removing artist popularity

**Answer:** It makes it worse, but surprisingly not by much
"""

# Change normal process by dropping artist popularity as well
X = df_audio.drop(columns=["song_popularity", 'artist_popularity'])
y = df_audio[['song_popularity']]


# split into training and test_sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create linear regression object
linreg = linear_model.LinearRegression()

# Fit the model
linreg.fit(X_train, y_train)

# Made predictions
y_pred = linreg.predict(X_test)


# MSE and RMSE
print("Model w/out regularization and with Genre features\n")
print("MSE:", mean_squared_error(y_test, y_pred))
print("RMSE:", math.sqrt(mean_squared_error(y_test, y_pred)))
print("R2 score:", linreg.score(X_test, y_test))

"""## LinReg w/ PCA
- PCA just barely outperforms the 2nd best model
- I tested several values for n_components and n=39 is the best. Note there are 41 features
"""

X = df_audio.drop(columns=["song_popularity"])
y = df_audio[['song_popularity']]

# split into training and test_sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Standardize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# Apply PCA
# pca = PCA(n_components='mle', svd_solver='full')
pca = PCA(n_components=39)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)


# Create linear regression object
linreg = linear_model.LinearRegression()

# Fit the Lin Reg model
linreg.fit(X_train_pca, y_train)

# Made predictions
y_pred = linreg.predict(X_test_pca)


# MSE and RMSE
print("Model w/out regularization and with Genre features\n")
print("MSE:", mean_squared_error(y_test, y_pred))
print("RMSE:", math.sqrt(mean_squared_error(y_test, y_pred)))
print("R2 score:", linreg.score(X_test_pca, y_test))

"""# Lyrics Classifier"""

from google.colab import drive
drive.mount('/content/drive')

#preprocess the lyrics and count the frequencies

corpus = []
y_data = []
y_lin_data = []
names = []
fileNames = ["SpotifyFeaturesLyricsEdited0.csv", "SpotifyFeaturesLyricsEdited1.csv", "SpotifyFeaturesLyricsEdited2.csv",\
             "SpotifyFeaturesLyricsEdited3.csv", "SpotifyFeaturesLyricsEdited4.csv", "SpotifyFeaturesLyricsEdited5.csv",\
             "SpotifyFeaturesLyricsEdited5.csv", "SpotifyFeaturesLyricsEdited6.csv", "SpotifyFeaturesLyricsEdited7.csv",\
             "SpotifyFeaturesLyricsEdited8.csv", "SpotifyFeaturesLyricsEdited9.csv", "SpotifyFeaturesLyricsEdited10.csv",\
             "SpotifyFeaturesLyricsEdited11.csv", "SpotifyFeaturesLyricsEdited12.csv", "SpotifyFeaturesLyricsEdited13.csv",\
             "SpotifyFeaturesLyricsEdited14.csv", "SpotifyFeaturesLyricsEdited15.csv"]
# "SpotifyFeaturesLyricsEdited16.csv",\
# "SpotifyFeaturesLyricsEdited17.csv"

total = 0
for name in fileNames:

  filepath = "/content/drive/MyDrive/clean/" + name

  # Read a single csv
  df = pd.read_csv(filepath)

  # for each row
  for (index, row) in df.iterrows():
    
    # tokenize lyrics into words and lower it
    lyr = nltk.word_tokenize(row['lyrics'])
    lyr = [l.lower() for l in lyr if l.isalpha()]
    
    
    y_data.append(row['popularity'])
    y_lin_data.append(row['popularity'])
    str1 = " "
    lyr = str1.join(lyr)
    corpus.append(lyr)
    names.append(row['track_name'])
  # print(df['popularity'].mean()) # print average song popularity for the file
  total = total + df['popularity'].mean()

threshold = 55
for i in range(len(y_data)):
  if y_data[i] >= threshold:
    y_data[i] = 1
  else:
    y_data[i] = 0

vectorizer = CountVectorizer(lowercase=True, strip_accents="ascii")
X_data = vectorizer.fit_transform(corpus)
# print("Avg: ", total / len(fileNames))
names = {"track_name": names}
namesDf = pd.DataFrame(data=names)

X_lin, X_lin_test, y_lin, y_lin_test = train_test_split(X_data, y_lin_data, test_size = 0.2, random_state=42)


#lin regression
lin_reg = ElasticNet(alpha=.5, l1_ratio = 0.4)
lin_reg = lin_reg.fit(X_lin, y_lin)
y_lin_pred = lin_reg.predict(X_lin_test)
print(lin_reg.score(X_lin_test, y_lin_test))
print(mean_squared_error(y_lin_test, y_lin_pred))
for i in range(len(y_lin_test)):
  print(y_lin_test[i], y_lin_pred[i])

X_lin

"""# Summary
- We are abandoning the ensemble due to the difficulties described at the beginning.
- The best Audio model tested above is PCA LinReg WITH genre features and W/O any regularization
- 2nd best model is same as above but without PCA. 
- Previous literature suggested we would get a big boost from using a social influence feature like artist popularity. And while we did get a boost, its less than I expected
- Adding regularization hurts perforamnce across the board
- Models perform much worse when genre features are dropped. One of the previous studies said their models suffered when they had too many features, but for us-- more features was better

"""